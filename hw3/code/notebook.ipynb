{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqtgqmuWD0hZ"
   },
   "source": [
    "# 1470 final project notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3uspvkrjEIaJ",
    "outputId": "9ae97a9e-721e-47e5-96f2-26c50a73256b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.10 (v3.9.10:f2f3f53782, Jan 13 2022, 17:02:14) \n",
      "[Clang 6.0 (clang-600.0.57)]\n"
     ]
    }
   ],
   "source": [
    "!python3 -VV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJJyIWTV9Yo_"
   },
   "source": [
    "Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Y7s9UO2FGug0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8x8EOJ099bO5"
   },
   "source": [
    "Code Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6DRRrF7J9kma"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import   assignment, conv_model, layers_keras, layers_manual\n",
    "%aimport assignment, conv_model, layers_keras, layers_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iji6pCKuut8R"
   },
   "source": [
    "Data Pathing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eehxvvoS-bP4"
   },
   "source": [
    "## Data Preprocessing: IAUFD !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "fl_FtgmC-fxl",
    "outputId": "aee17b99-97ea-483e-ddd2-3f57a53cfa1a"
   },
   "outputs": [],
   "source": [
    "#data = assignment.get_data()\n",
    "#X0, Y0, X1, Y1, D0, D1, D_info = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file '../data/imagesLocal/.DS_Store'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m/Users/amolsanghera/Desktop/CS1470/soccer-image-captioning/hw3/code/notebook.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/amolsanghera/Desktop/CS1470/soccer-image-captioning/hw3/code/notebook.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data \u001b[39m=\u001b[39m assignment\u001b[39m.\u001b[39;49mdivide_data()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/amolsanghera/Desktop/CS1470/soccer-image-captioning/hw3/code/notebook.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m X0, Y0, X1, Y1 \u001b[39m=\u001b[39m data\n",
      "File \u001b[0;32m~/Desktop/CS1470/soccer-image-captioning/hw3/code/assignment.py:46\u001b[0m, in \u001b[0;36mdivide_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdivide_data\u001b[39m():\n\u001b[1;32m     45\u001b[0m     labels \u001b[39m=\u001b[39m preprocess_labels()\n\u001b[0;32m---> 46\u001b[0m     images \u001b[39m=\u001b[39m images_in_array()\n\u001b[1;32m     49\u001b[0m     X0 \u001b[39m=\u001b[39m images[\u001b[39m0\u001b[39m:\u001b[39m10\u001b[39m]\n\u001b[1;32m     50\u001b[0m     Y0 \u001b[39m=\u001b[39m labels[\u001b[39m0\u001b[39m:\u001b[39m10\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/CS1470/soccer-image-captioning/hw3/code/assignment.py:33\u001b[0m, in \u001b[0;36mimages_in_array\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimages_in_array\u001b[39m():\n\u001b[1;32m     31\u001b[0m     image_folder_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../data/imagesLocal/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 33\u001b[0m     images \u001b[39m=\u001b[39m [imread(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(image_folder_path, f)) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(image_folder_path)]\n\u001b[1;32m     34\u001b[0m     \u001b[39mif\u001b[39;00m image_folder_path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.DS_Store\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m images:\n\u001b[1;32m     35\u001b[0m         images\u001b[39m.\u001b[39mremove(image_folder_path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.DS_Store\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/CS1470/soccer-image-captioning/hw3/code/assignment.py:33\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimages_in_array\u001b[39m():\n\u001b[1;32m     31\u001b[0m     image_folder_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../data/imagesLocal/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 33\u001b[0m     images \u001b[39m=\u001b[39m [imread(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(image_folder_path, f)) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(image_folder_path)]\n\u001b[1;32m     34\u001b[0m     \u001b[39mif\u001b[39;00m image_folder_path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.DS_Store\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m images:\n\u001b[1;32m     35\u001b[0m         images\u001b[39m.\u001b[39mremove(image_folder_path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.DS_Store\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/matplotlib/pyplot.py:2111\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   2109\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(matplotlib\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mimread)\n\u001b[1;32m   2110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimread\u001b[39m(fname, \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 2111\u001b[0m     \u001b[39mreturn\u001b[39;00m matplotlib\u001b[39m.\u001b[39;49mimage\u001b[39m.\u001b[39;49mimread(fname, \u001b[39mformat\u001b[39;49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/matplotlib/image.py:1541\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1534\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fname, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(parse\u001b[39m.\u001b[39murlparse(fname)\u001b[39m.\u001b[39mscheme) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1535\u001b[0m     \u001b[39m# Pillow doesn't handle URLs directly.\u001b[39;00m\n\u001b[1;32m   1536\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1537\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease open the URL for reading and pass the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1538\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mresult to Pillow, e.g. with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1539\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m``np.array(PIL.Image.open(urllib.request.urlopen(url)))``.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1540\u001b[0m         )\n\u001b[0;32m-> 1541\u001b[0m \u001b[39mwith\u001b[39;00m img_open(fname) \u001b[39mas\u001b[39;00m image:\n\u001b[1;32m   1542\u001b[0m     \u001b[39mreturn\u001b[39;00m (_pil_png_to_float_array(image)\n\u001b[1;32m   1543\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(image, PIL\u001b[39m.\u001b[39mPngImagePlugin\u001b[39m.\u001b[39mPngImageFile) \u001b[39melse\u001b[39;00m\n\u001b[1;32m   1544\u001b[0m             pil_to_array(image))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/PIL/Image.py:3123\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3121\u001b[0m \u001b[39mfor\u001b[39;00m message \u001b[39min\u001b[39;00m accept_warnings:\n\u001b[1;32m   3122\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(message)\n\u001b[0;32m-> 3123\u001b[0m \u001b[39mraise\u001b[39;00m UnidentifiedImageError(\n\u001b[1;32m   3124\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcannot identify image file \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (filename \u001b[39mif\u001b[39;00m filename \u001b[39melse\u001b[39;00m fp)\n\u001b[1;32m   3125\u001b[0m )\n",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file '../data/imagesLocal/.DS_Store'"
     ]
    }
   ],
   "source": [
    "data = assignment.divide_data()\n",
    "X0, Y0, X1, Y1 = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print all labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmenting train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-07 15:27:03.153547: W tensorflow/core/framework/op_kernel.cc:1722] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Cast string to float is not supported [Op:Cast]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [65], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m## You can use any list of 10 indices\u001b[39;00m\n\u001b[1;32m      4\u001b[0m sample_image_indices \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m6\u001b[39m, \u001b[39m7\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m9\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m sample_images \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mcast(tf\u001b[39m.\u001b[39;49mgather(X0, sample_image_indices), tf\u001b[39m.\u001b[39;49mfloat32)\n\u001b[1;32m      6\u001b[0m sample_labels \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mgather(Y0, sample_image_indices)\n\u001b[1;32m      8\u001b[0m fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m3\u001b[39m, \u001b[39m10\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/dl3/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:7107\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7105\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7106\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 7107\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: Cast string to float is not supported [Op:Cast]"
     ]
    }
   ],
   "source": [
    "import conv_model\n",
    "\n",
    "## You can use any list of 10 indices\n",
    "sample_image_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "sample_images = tf.cast(tf.gather(X0, sample_image_indices), tf.float32)\n",
    "sample_labels = tf.gather(Y0, sample_image_indices)\n",
    "\n",
    "fig, ax = plt.subplots(3, 10)\n",
    "fig.set_size_inches(24, 8)\n",
    "\n",
    "args = conv_model.get_default_CNN_model()\n",
    "\n",
    "preprocessed_images = args.model.input_prep_fn(sample_images)\n",
    "augmented_images = args.model.augment_fn(preprocessed_images)\n",
    "\n",
    "for i in range(10):\n",
    "    ax[0][i].imshow(sample_images[i]/255., cmap = \"Greys\")\n",
    "    ax[1][i].imshow(preprocessed_images[i], cmap = \"Greys\")\n",
    "    ax[2][i].imshow(augmented_images[i], cmap = \"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train A Basic Keras Model\n",
    "\n",
    "As part of step 2 from the handout, we just want you to construct a simple keras model to run prediction on your dataset!\n",
    "\n",
    "Implement **[TODO 2]** in `get_default_CNN_model` to return a CNN model that can train above an accuracy of 55% (note that the requirement for 1470 is 62% and for 2470, is 65% though). Feel free to play around with the number of layers, hyperparameters for layers, epochs, batch size, and anything else you can think of. \n",
    "\n",
    "**Requirements:**\n",
    "- Model must contain Conv2D, BatchNormalization, and Dropout layers. \n",
    "- These must be imported from the argument namespaces (already done by default).\n",
    "- Task 1 will automatically use `tf.keras.layers` implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 7, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [45], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39massignment\u001b[39;00m \n\u001b[1;32m      3\u001b[0m \u001b[39m## You can test with more epochs later\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m cnn_model \u001b[39m=\u001b[39m assignment\u001b[39m.\u001b[39;49mrun_task(data, \u001b[39m1\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m250\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/files/brown-cs/cs1470/soccer-image-captioning/hw3/code/assignment.py:120\u001b[0m, in \u001b[0;36mrun_task\u001b[0;34m(data, task, subtask, epochs, batch_size)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlayers_manual\u001b[39;00m  \u001b[39m## Where your manual non-diffable conv implementation resides\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[39m## Retrieve data from tuple\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m X0, Y0, X1, Y1, D0, D1, D_info \u001b[39m=\u001b[39m data\n\u001b[1;32m    122\u001b[0m subtask \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m] \u001b[39mif\u001b[39;00m subtask \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m [\u001b[39mint\u001b[39m(subtask)]\n\u001b[1;32m    124\u001b[0m \u001b[39m## Get a working model with regular tf.keras.layers components\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[39m## when task = 1.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 7, got 4)"
     ]
    }
   ],
   "source": [
    "import assignment \n",
    "\n",
    "## You can test with more epochs later\n",
    "cnn_model = assignment.run_task(data, 1, epochs=10, batch_size=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 7, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [46], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39massignment\u001b[39;00m \n\u001b[1;32m      3\u001b[0m \u001b[39m## You can test with more epochs later\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m cnn_model \u001b[39m=\u001b[39m assignment\u001b[39m.\u001b[39;49mrun_task(data, \u001b[39m2\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/files/brown-cs/cs1470/soccer-image-captioning/hw3/code/assignment.py:120\u001b[0m, in \u001b[0;36mrun_task\u001b[0;34m(data, task, subtask, epochs, batch_size)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlayers_manual\u001b[39;00m  \u001b[39m## Where your manual non-diffable conv implementation resides\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[39m## Retrieve data from tuple\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m X0, Y0, X1, Y1, D0, D1, D_info \u001b[39m=\u001b[39m data\n\u001b[1;32m    122\u001b[0m subtask \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m] \u001b[39mif\u001b[39;00m subtask \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m [\u001b[39mint\u001b[39m(subtask)]\n\u001b[1;32m    124\u001b[0m \u001b[39m## Get a working model with regular tf.keras.layers components\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[39m## when task = 1.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 7, got 4)"
     ]
    }
   ],
   "source": [
    "import assignment \n",
    "\n",
    "## You can test with more epochs later\n",
    "cnn_model = assignment.run_task(data, 2, epochs=10, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cnn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cnn_model\u001b[39m.\u001b[39msummary()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cnn_model' is not defined"
     ]
    }
   ],
   "source": [
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Your Own Layers\n",
    "For steps 3, 4, and 5 from the handout, you'll need to implement the layers from scratch inside of `layers_keras.py`.\n",
    "Feel free to refer to the official documentation for how these methods are supposed to function. \n",
    "More details are included in the layer block comments, and the init methods are already provided. \n",
    "\n",
    "**Requirements**:\n",
    "- Implement Conv2D, BatchNormalization, and Dropout in `layers_keras.py`\n",
    "- Cannot use existing layers as sub-components. \n",
    "- Cannot use `tf.nn.batch_normalization` or `tf.nn.dropout`. \n",
    "- CAN use `tf.nn.convolution`...\n",
    "- Should utilize all non-commented-out arguments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the below code block to confirm that your custom implementation of Conv2D runs without erroring. This does not guarantee that your forward pass calculations are correct. It serves only as a preliminary check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tf.Tensor(\n",
      "[[[[-1.3874068]\n",
      "   [ 1.8371265]]\n",
      "\n",
      "  [[ 4.395982 ]\n",
      "   [ 5.2402725]]]], shape=(1, 2, 2, 1), dtype=float32)\n",
      "Expected: tf.Tensor(\n",
      "[[[[-1.3874068]\n",
      "   [ 1.8371265]]\n",
      "\n",
      "  [[ 4.395982 ]\n",
      "   [ 5.2402725]]]], shape=(1, 2, 2, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import layers_keras\n",
    "\n",
    "random_input = tf.random.uniform((1, 4, 4, 3), 0, 10, dtype=tf.float32)\n",
    "\n",
    "seed = 8675309\n",
    "tf.random.set_seed(seed)\n",
    "conv_layer = layers_keras.Conv2D(1, 2, strides=2)\n",
    "print(\"Output:\", conv_layer(random_input, training=True))\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "conv_layer = tf.keras.layers.Conv2D(1, 2, strides=2)\n",
    "print('Expected:', conv_layer(random_input, training=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the below code block to confirm that your custom implementation of Batch Normalization runs without erroring. This does not guarantee that your forward pass calculations are correct. It serves only as a preliminary check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tf.Tensor(\n",
      "[[4.9200354  4.1594877  0.15996099]\n",
      " [0.17104864 3.8494146  4.5375834 ]\n",
      " [3.0904114  8.937246   6.2552547 ]], shape=(3, 3), dtype=float32)\n",
      "Output: tf.Tensor(\n",
      "[[ 1.1211212  -0.6394283  -1.3602844 ]\n",
      " [-1.3068336  -0.7725641   0.3454902 ]\n",
      " [ 0.18571232  1.4119927   1.0147942 ]], shape=(3, 3), dtype=float32)\n",
      "Expected: tf.Tensor(\n",
      "[[ 1.1211214  -0.6394284  -1.3602844 ]\n",
      " [-1.3068336  -0.7725642   0.3454901 ]\n",
      " [ 0.18571234  1.4119925   1.0147942 ]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import layers_keras\n",
    "\n",
    "random_input = tf.random.uniform((3,3), 0, 10, dtype=tf.float32)\n",
    "print(\"Input:\", random_input)\n",
    "\n",
    "batch_norm = layers_keras.BatchNormalization()\n",
    "print(\"Output:\", batch_norm(random_input, training=True))\n",
    "\n",
    "batch_norm = tf.keras.layers.BatchNormalization()\n",
    "print('Expected:', batch_norm(random_input, training=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the below code block to confirm that your custom implementation of Dropout runs without erroring. This does not guarantee that your forward pass or input gradients calculations are correct. It serves only as a preliminary check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " tf.Tensor(\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]], shape=(2, 11), dtype=float32)\n",
      "\n",
      "Output Training:\n",
      "tf.Tensor(\n",
      "[[1.25 1.25 1.25 1.25 1.25 1.25 0.   1.25 1.25 1.25 0.  ]\n",
      " [1.25 1.25 1.25 1.25 1.25 0.   1.25 0.   0.   1.25 1.25]], shape=(2, 11), dtype=float32)\n",
      "Expected Training:\n",
      "tf.Tensor(\n",
      "[[1.25 1.25 1.25 1.25 1.25 1.25 0.   1.25 1.25 1.25 0.  ]\n",
      " [1.25 1.25 1.25 1.25 1.25 0.   1.25 0.   0.   1.25 1.25]], shape=(2, 11), dtype=float32)\n",
      "\n",
      "Output Testing:\n",
      "tf.Tensor(\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]], shape=(2, 11), dtype=float32)\n",
      "Expected Testing:\n",
      "tf.Tensor(\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]], shape=(2, 11), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import layers_keras\n",
    "\n",
    "random_input = tf.ones((2, 11))\n",
    "print(\"Input:\\n\", random_input)\n",
    "\n",
    "seed = 8675309\n",
    "for mode_str, mode in zip(['Training', 'Testing'], [True, False]):\n",
    "    print()\n",
    "    for layer_str, layer in zip(['Output','Expected'], [layers_keras.Dropout, tf.keras.layers.Dropout]):\n",
    "        tf.random.set_seed(seed)\n",
    "        dropout_fn = layer(rate=0.2)\n",
    "        print(f'{layer_str} {mode_str}:')\n",
    "        print(dropout_fn(random_input, training=mode))\n",
    "\n",
    "# Expected: Around rate% of the entries should be zeros in training mode.\n",
    "#   Should also be normalized such that, on average, magnitude perserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training your model\n",
    "Now, let's see if your model works with the new components in place?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 7, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [51], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39massignment\u001b[39;00m \n\u001b[1;32m      3\u001b[0m \u001b[39m# assignment.run_task(data, 2, 1, epochs=2)   ## Just manual conv\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# assignment.run_task(data, 2, 2, epochs=2)   ## Just manual bnorm\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# assignment.run_task(data, 2, 3, epochs=2)   ## Just manual dropout\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m assignment\u001b[39m.\u001b[39;49mrun_task(data, \u001b[39m2\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/files/brown-cs/cs1470/soccer-image-captioning/hw3/code/assignment.py:120\u001b[0m, in \u001b[0;36mrun_task\u001b[0;34m(data, task, subtask, epochs, batch_size)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlayers_manual\u001b[39;00m  \u001b[39m## Where your manual non-diffable conv implementation resides\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[39m## Retrieve data from tuple\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m X0, Y0, X1, Y1, D0, D1, D_info \u001b[39m=\u001b[39m data\n\u001b[1;32m    122\u001b[0m subtask \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m] \u001b[39mif\u001b[39;00m subtask \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m [\u001b[39mint\u001b[39m(subtask)]\n\u001b[1;32m    124\u001b[0m \u001b[39m## Get a working model with regular tf.keras.layers components\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[39m## when task = 1.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 7, got 4)"
     ]
    }
   ],
   "source": [
    "import assignment \n",
    "\n",
    "# assignment.run_task(data, 2, 1, epochs=2)   ## Just manual conv\n",
    "# assignment.run_task(data, 2, 2, epochs=2)   ## Just manual bnorm\n",
    "# assignment.run_task(data, 2, 3, epochs=2)   ## Just manual dropout\n",
    "assignment.run_task(data, 2, epochs=2)        ## Test all 3!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Convolution!\n",
    "\n",
    "Now, go ahead and implement convolution manually! This should be done inside of the `layers_manual.py` file. It's very non-trivial to perform convolution differentiably without using `tf.nn.convolution`, so the manual convolution should only run during inference time. Below is a quick test to see if your convolution is consistent with the Keras layered version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tf.Tensor(\n",
      "[[[[ 4.985508  ]\n",
      "   [ 0.72873664]]\n",
      "\n",
      "  [[ 3.8827434 ]\n",
      "   [-1.1475484 ]]]\n",
      "\n",
      "\n",
      " [[[-3.088588  ]\n",
      "   [-1.837323  ]]\n",
      "\n",
      "  [[-5.2728376 ]\n",
      "   [ 0.12673473]]]], shape=(2, 2, 2, 1), dtype=float32)\n",
      "Expected: tf.Tensor(\n",
      "[[[[ 4.9855075 ]\n",
      "   [ 0.72873664]]\n",
      "\n",
      "  [[ 3.8827434 ]\n",
      "   [-1.1475487 ]]]\n",
      "\n",
      "\n",
      " [[[-3.0885882 ]\n",
      "   [-1.8373222 ]]\n",
      "\n",
      "  [[-5.272838  ]\n",
      "   [ 0.12673473]]]], shape=(2, 2, 2, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import layers_manual\n",
    "\n",
    "random_input = tf.random.uniform((2, 4, 4, 3), 0, 10, dtype=tf.float32)\n",
    "\n",
    "seed = 8675309\n",
    "tf.random.set_seed(seed)\n",
    "conv_layer = layers_manual.Conv2D(1, (2, 2), strides=2, padding='valid')\n",
    "print(\"Output:\", conv_layer(random_input, training=False))\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "conv_layer = tf.keras.layers.Conv2D(1, (2, 2), strides=2, padding='valid')\n",
    "print('Expected:', conv_layer(random_input, training=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the loop, this will happen at the end of every epoch because a validation set is being evaluated alongside your training set. The following will test it out for you! Don't worry if your categorical accuracy looks low here. As long as everything works without erroring, feel free to move on and test the whole model together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 7, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [53], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39massignment\u001b[39;00m \n\u001b[0;32m----> 3\u001b[0m assignment\u001b[39m.\u001b[39;49mrun_task(data, \u001b[39m3\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/files/brown-cs/cs1470/soccer-image-captioning/hw3/code/assignment.py:120\u001b[0m, in \u001b[0;36mrun_task\u001b[0;34m(data, task, subtask, epochs, batch_size)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlayers_manual\u001b[39;00m  \u001b[39m## Where your manual non-diffable conv implementation resides\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[39m## Retrieve data from tuple\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m X0, Y0, X1, Y1, D0, D1, D_info \u001b[39m=\u001b[39m data\n\u001b[1;32m    122\u001b[0m subtask \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m] \u001b[39mif\u001b[39;00m subtask \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m [\u001b[39mint\u001b[39m(subtask)]\n\u001b[1;32m    124\u001b[0m \u001b[39m## Get a working model with regular tf.keras.layers components\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[39m## when task = 1.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 7, got 4)"
     ]
    }
   ],
   "source": [
    "import assignment \n",
    "\n",
    "assignment.run_task(data, 3, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping Up\n",
    "\n",
    "Make sure your model runs and trains up to standards! When you find a model configuration that you like, feel free to update your `get_default_CNN_model` function so that the autograder can use it with your arguments. If your model takes too long to train (> 10 mins), the autograder may time out, so take consideration of that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run at least once\n",
    "from types import SimpleNamespace\n",
    "from conv_model import CustomSequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, you can copy your code here for quick testing!\n",
    "\n",
    "Make sure to put it back into your `conv_model.py` file for the autograder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_CNN_model(\n",
    "    conv_ns=tf.keras.layers,\n",
    "    norm_ns=tf.keras.layers,\n",
    "    drop_ns=tf.keras.layers,\n",
    "    man_conv_ns=tf.keras.layers,\n",
    "):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [56], line 13\u001b[0m\n\u001b[1;32m      4\u001b[0m man_conv_ns \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\n\u001b[1;32m      6\u001b[0m args \u001b[39m=\u001b[39m get_default_CNN_model(\n\u001b[1;32m      7\u001b[0m     conv_ns\u001b[39m=\u001b[39mconv_ns, \n\u001b[1;32m      8\u001b[0m     norm_ns\u001b[39m=\u001b[39mnorm_ns, \n\u001b[1;32m      9\u001b[0m     drop_ns\u001b[39m=\u001b[39mdrop_ns, \n\u001b[1;32m     10\u001b[0m     man_conv_ns\u001b[39m=\u001b[39mman_conv_ns\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m history \u001b[39m=\u001b[39m args\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39mfit(\n\u001b[1;32m     14\u001b[0m     X0,\n\u001b[1;32m     15\u001b[0m     Y0,\n\u001b[1;32m     16\u001b[0m     epochs\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mepochs,\n\u001b[1;32m     17\u001b[0m     batch_size\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mbatch_size,\n\u001b[1;32m     18\u001b[0m     validation_data\u001b[39m=\u001b[39m(X1, Y1),\n\u001b[1;32m     19\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "conv_ns = tf.keras.layers\n",
    "norm_ns = tf.keras.layers\n",
    "drop_ns = tf.keras.layers\n",
    "man_conv_ns = tf.keras.layers\n",
    "\n",
    "args = get_default_CNN_model(\n",
    "    conv_ns=conv_ns, \n",
    "    norm_ns=norm_ns, \n",
    "    drop_ns=drop_ns, \n",
    "    man_conv_ns=man_conv_ns\n",
    ")\n",
    "\n",
    "history = args.model.fit(\n",
    "    X0,\n",
    "    Y0,\n",
    "    epochs=args.epochs,\n",
    "    batch_size=args.batch_size,\n",
    "    validation_data=(X1, Y1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Checks\n",
    "\n",
    "In case you need them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cnn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m P1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(cnn_model\u001b[39m.\u001b[39mpredict(X1), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      2\u001b[0m confusion_mtx \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mconfusion_matrix(P1, Y1)\n\u001b[1;32m      4\u001b[0m P0 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(cnn_model\u001b[39m.\u001b[39mpredict(X0), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cnn_model' is not defined"
     ]
    }
   ],
   "source": [
    "P1 = np.argmax(cnn_model.predict(X1), -1)\n",
    "confusion_mtx = tf.math.confusion_matrix(P1, Y1)\n",
    "\n",
    "P0 = np.argmax(cnn_model.predict(X0), -1)\n",
    "confusion_mtx = tf.math.confusion_matrix(P0, Y0)\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.imshow(confusion_mtx, cmap='hot', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cnn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m P1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(cnn_model\u001b[39m.\u001b[39mpredict(X1), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      2\u001b[0m confusion_mtx \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mconfusion_matrix(P1, Y1)\n\u001b[1;32m      3\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m12\u001b[39m, \u001b[39m9\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cnn_model' is not defined"
     ]
    }
   ],
   "source": [
    "P1 = np.argmax(cnn_model.predict(X1), -1)\n",
    "confusion_mtx = tf.math.confusion_matrix(P1, Y1)\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.imshow(confusion_mtx, cmap='hot', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cnn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [59], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m2\u001b[39m, \u001b[39m10\u001b[39m)\n\u001b[1;32m      2\u001b[0m fig\u001b[39m.\u001b[39mset_size_inches(\u001b[39m24\u001b[39m, \u001b[39m8\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m pred0 \u001b[39m=\u001b[39m cnn_model\u001b[39m.\u001b[39mpredict(X0[:\u001b[39m10\u001b[39m])\n\u001b[1;32m      5\u001b[0m pred1 \u001b[39m=\u001b[39m cnn_model\u001b[39m.\u001b[39mpredict(X1[:\u001b[39m10\u001b[39m])\n\u001b[1;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mp2l\u001b[39m(pred):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cnn_model' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2400x800 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(2, 10)\n",
    "fig.set_size_inches(24, 8)\n",
    "\n",
    "pred0 = cnn_model.predict(X0[:10])\n",
    "pred1 = cnn_model.predict(X1[:10])\n",
    "\n",
    "def p2l(pred):\n",
    "    return D_info.features['label']._int2str[pred]\n",
    "\n",
    "for i in range(10):\n",
    "    ax[0][i].imshow(X0[i], cmap = \"Greys\")\n",
    "    ax[1][i].imshow(X1[i], cmap = \"Greys\")\n",
    "    ax[1][i].tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "    ax[0][i].set_xlabel(f\"Pred {p2l(np.argmax(pred0[i], -1))} | {p2l(Y0[i])}\")    \n",
    "    ax[1][i].set_xlabel(f\"Pred {p2l(np.argmax(pred1[i], -1))} | {p2l(Y1[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.4 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.4 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.4 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.4 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.4 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.4 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW3_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
